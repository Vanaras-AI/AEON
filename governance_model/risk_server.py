#!/usr/bin/env python3
"""
Gemma 3 270M Governance Risk Scoring Server
Fine-tuned on GRC (Governance, Risk, Compliance) with LoRA adapter
"""

import torch
import json
import sys
import re
from http.server import BaseHTTPRequestHandler, HTTPServer
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

PORT = 8001
BASE_MODEL_ID = "google/gemma-3-270m-it"
ADAPTER_PATH = "./adapter"

# Security-focused system prompt for risk scoring
SECURITY_SYSTEM_PROMPT = """You are AEON's Governance Cortex, a security expert analyzing AI agent tool calls.

Your task: Assess the security risk of each action and return a risk score.

## CRITICAL THREAT PATTERNS (Score 0.90-1.0):
- Remote Code Execution: curl|bash, wget|sh, piping to shell interpreter
- Credential Exposure: Writing private keys, passwords, API tokens
- System Tampering: Modifying /etc/*, /usr/bin/*, system configs
- Permission Escalation: chmod 777, setuid, setgid
- Data Exfiltration: tar + network, encrypting files, uploading to external

## HIGH RISK PATTERNS (Score 0.70-0.89):
- Network Downloads: wget, curl downloading executables
- Sensitive File Access: Reading .ssh/*, .env, credentials
- Process Manipulation: kill, pkill system processes
- Reverse Shells: nc -e, bash -i, python reverse shell

## MEDIUM RISK PATTERNS (Score 0.40-0.69):
- File Operations: Writing to user directories
- Safe Network: API calls to known endpoints
- Build Commands: npm, pip, cargo

## LOW RISK PATTERNS (Score 0.00-0.39):
- Read-only: cat, ls, echo, pwd
- Safe Writes: /tmp/*, logs, user workspace

Respond with ONLY valid JSON:
{"risk_score": <float 0.0-1.0>, "risk_level": "<CRITICAL|HIGH|MEDIUM|LOW>", "threats": ["<threat1>", ...], "reasoning": "<brief explanation>"}
"""


class GovernanceModel:
    def __init__(self):
        print("üß† [GEMMA] Loading Governance SLM...", file=sys.stderr)
        try:
            self.base_model = AutoModelForCausalLM.from_pretrained(
                BASE_MODEL_ID,
                torch_dtype=torch.float32,
                device_map="auto",
                trust_remote_code=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
            
            print(f"üîß [GEMMA] Loading LoRA adapter from {ADAPTER_PATH}...", file=sys.stderr)
            self.model = PeftModel.from_pretrained(self.base_model, ADAPTER_PATH)
            self.model.eval()
            
            print("‚úÖ [GEMMA] Governance SLM ready", file=sys.stderr)
        except Exception as e:
            print(f"‚ùå [GEMMA] Failed to load: {e}", file=sys.stderr)
            raise

    def score_risk(self, method: str, params: dict) -> dict:
        """Score risk using governance-trained Gemma with security prompt"""
        
        # Build action description
        action_desc = f"Tool: {method}\n"
        if "command" in params:
            action_desc += f"Command: {params['command']}\n"
        if "path" in params:
            action_desc += f"Path: {params['path']}\n"
        if "content" in params:
            content_preview = params["content"][:200] + "..." if len(params.get("content", "")) > 200 else params.get("content", "")
            action_desc += f"Content Preview: {content_preview}\n"
        
        # Format prompt for Gemma
        formatted_prompt = (
            f"<start_of_turn>user\n"
            f"{SECURITY_SYSTEM_PROMPT}\n\n"
            f"Analyze this AI agent action:\n{action_desc}<end_of_turn>\n"
            f"<start_of_turn>model\n"
        )
        
        inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
        input_length = inputs.input_ids.shape[1]
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=False,  # Deterministic for security
                temperature=0.1
            )
        
        # Get ONLY the new tokens (skip the prompt)
        new_tokens = outputs[0][input_length:]
        response_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
        
        print(f"   Model response: {response_text[:100]}...", file=sys.stderr)
        
        # Try to parse JSON
        try:
            # Find JSON in response
            json_match = re.search(r'\{[^{}]*\}', response_text)
            if json_match:
                result = json.loads(json_match.group())
                # Ensure required fields
                score = float(result.get("risk_score", 0.5))
                return {
                    "risk_score": min(1.0, max(0.0, score)),
                    "risk_level": result.get("risk_level", self._score_to_level(score)),
                    "threats": result.get("threats", []),
                    "reasoning": result.get("reasoning", "")
                }
        except (json.JSONDecodeError, ValueError) as e:
            print(f"‚ö†Ô∏è  [GEMMA] JSON parse error: {e}", file=sys.stderr)
        
        # Fallback: extract score from text
        score_match = re.search(r'(?:risk[_\s]?score|score)[:\s]*([0-9]+\.?[0-9]*)', response_text.lower())
        if score_match:
            try:
                score = float(score_match.group(1))
                return {
                    "risk_score": min(1.0, max(0.0, score)),
                    "risk_level": self._score_to_level(score),
                    "threats": [],
                    "reasoning": response_text[:200]
                }
            except ValueError:
                pass
        
        # Look for risk level keywords
        response_lower = response_text.lower()
        if "critical" in response_lower or "danger" in response_lower:
            inferred_score = 0.95
        elif "high" in response_lower:
            inferred_score = 0.75
        elif "medium" in response_lower or "moderate" in response_lower:
            inferred_score = 0.5
        elif "low" in response_lower or "minimal" in response_lower or "safe" in response_lower:
            inferred_score = 0.2
        else:
            inferred_score = 0.5  # Default uncertain
        
        print(f"‚ö†Ô∏è  [GEMMA] Inferred score from text: {inferred_score}", file=sys.stderr)
        return {
            "risk_score": inferred_score,
            "risk_level": self._score_to_level(inferred_score),
            "threats": [],
            "reasoning": response_text[:200]
        }
    
    def _score_to_level(self, score: float) -> str:
        if score >= 0.9:
            return "CRITICAL"
        elif score >= 0.7:
            return "HIGH"
        elif score >= 0.4:
            return "MEDIUM"
        else:
            return "LOW"


# Global model
MODEL = None

class InferenceHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        global MODEL
        
        if self.path == '/score_risk':
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            
            try:
                data = json.loads(post_data)
                method = data.get("method", "unknown")
                params = data.get("params", {})
                
                print(f"üéØ [GEMMA] Scoring: {method}", file=sys.stderr)
                
                result = MODEL.score_risk(method, params)
                
                print(f"   ‚Üí {result['risk_score']:.2f} ({result['risk_level']})", file=sys.stderr)
                if result.get('threats'):
                    print(f"   ‚Üí Threats: {result['threats']}", file=sys.stderr)
                
                self.send_response(200)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(result).encode('utf-8'))
                
            except Exception as e:
                print(f"‚ùå [ERROR] {e}", file=sys.stderr)
                self.send_response(500)
                self.end_headers()
                self.wfile.write(json.dumps({"error": str(e)}).encode('utf-8'))
        else:
            self.send_response(404)
            self.end_headers()
    
    def log_message(self, format, *args):
        return


if __name__ == "__main__":
    MODEL = GovernanceModel()
    
    HTTPServer.allow_reuse_address = True
    with HTTPServer(("", PORT), InferenceHandler) as httpd:
        print(f"üèõÔ∏è [GEMMA] Governance Risk Server on :{PORT}", file=sys.stderr)
        sys.stderr.flush()
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\nüõë Shutting down.", file=sys.stderr)
